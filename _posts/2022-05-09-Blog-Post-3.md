---
layout: post
title: Image Classification
---

## ยง1. Load Packages and Obtain Data

We start by importing modules:

```python
import os
import tensorflow as tf
from tensorflow.keras import utils, datasets, layers, losses
```

Next, we will obtain our data from a zip file:

```python
# location of data
_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

# download the data and extract it
path_to_zip = utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

# construct paths
PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

# parameters for datasets
BATCH_SIZE = 32
IMG_SIZE = (160, 160)

# construct train and validation datasets 
train_dataset = utils.image_dataset_from_directory(train_dir,
                                                   shuffle=True,
                                                   batch_size=BATCH_SIZE,
                                                   image_size=IMG_SIZE)

validation_dataset = utils.image_dataset_from_directory(validation_dir,
                                                        shuffle=True,
                                                        batch_size=BATCH_SIZE,
                                                        image_size=IMG_SIZE)

# construct the test dataset by taking every 5th observation out of the validation dataset
val_batches = tf.data.experimental.cardinality(validation_dataset)
test_dataset = validation_dataset.take(val_batches // 5)
validation_dataset = validation_dataset.skip(val_batches // 5)
```

And now we will fine tune the dataset.

```python
AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)
```

Now that we have the data, we can start by creating a visualization, such as the one below, which outputs one row of three cats, and one row of three dogs.

```python
import matplotlib.pyplot as plt
import numpy as np

def tworowvis():
  plt.figure(figsize=(10, 10))
  for images, labels in train_dataset.take(1):
    i = 0 # Set iterators to 0
    j = 0
    k = 0
    while i < 3: # Plot first row
      ax = plt.subplot(2, 3, i + 1)
      if int(labels[j]) == 0: # Only plot cats
        plt.imshow(images[j].numpy().astype("uint8"))
        plt.title("Cat")
        plt.axis("off")
        i += 1
      j += 1
    while i < 6: # Plot second row
      ax = plt.subplot(2, 3, i + 1)
      if int(labels[k]) == 1: # Only plot dogs
        plt.imshow(images[k].numpy().astype("uint8"))
        plt.title("Dog")
        plt.axis("off")
        i += 1
      k += 1

tworowvis() # Render visualization
```

![Blog-3-Sec-1-Vis.png]({{ site.baseurl }}/images/Blog-3-Sec-1-Vis.png)

Next, we will create our baseline for our model's accuracy by calculating the number of cats and dogs in the dataset, and using the higher proportion as our baseline.

```python
cat = 0
dog = 0
for im, lab in train_dataset:
  for i in lab:
    if int(lab[i]) == 0:
      cat += 1
    else:
      dog += 1
print(max(cat/2000, dog/2000))
```

The result of running this code is 0.5425, indicating a baseline of 54.25% accuracy.

## ยง2. First Model

We will create our first model using keras Sequential, and using many layers, including convolutional layers, max pooling layes, flattening, dropout, and dense layers.  These layers each perform different operations, as specified in the comments of the block below.

```python
model1 = tf.keras.Sequential([
 layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(32, (3, 3), activation='relu'),
 layers.MaxPooling2D((2, 2)), # take maximum of 2x2 blocks
 layers.Conv2D(64, (3, 3), activation='relu'), # n "pixels" x n "pixels" x 64
 layers.Flatten(), # n^2 * 64 length vector
 layers.Dropout(0.2), # randomly "drop" or delete 20% of connections between the previous layer and the next layer
 layers.Dropout(0.2),
 layers.Dense(64, activation='relu'),
 layers.Dense(2) # output 2 predictors
])
```

Next, we will compile our model.

```python
model1.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
```

Then, we use this code to fit the model.

```python
history = model1.fit(train_dataset, epochs = 20, validation_data = validation_dataset)
```

![Blog-3-m1-e1.png]({{ site.baseurl }}/images/Blog-3-m1-e1.png)
![Blog-3-m1-e2.png]({{ site.baseurl }}/images/Blog-3-m1-e2.png)

And this is a visualization of training and validation accuracy across epochs.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![Blog-3-m1-vis.png]({{ site.baseurl }}/images/Blog-3-m1-vis.png)

The accuracy of the model stabilized **between 60% and 65%** during training.  This is about 5-10% better than the baseline, so this model is moderately better than the baseline.  We observe very high overfitting in model1, as seen through our training accuracy, which ended up around 98%, so there is a gap of over 30%, which indicates very severe overfitting.

## ยง3. Model with Data Augmentation

Now we will use data augmentation, where we make more data points by rotating and flipping the images we have, since a rotated or flipped cat is still a cat.  Below is an example of some flipped cats.

```python
rflip = tf.keras.Sequential([layers.experimental.preprocessing.RandomFlip()])

for image, label in train_dataset.take(1):
  first_image = image[0]

image = tf.expand_dims(first_image, 0)

plt.figure(figsize=(10, 10))
for i in range(9):
  flipimage = rflip(image, training = True)
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(flipimage[0] / 255)
  plt.axis("off")
```

![Blog-3-Sec-3-Vis-1.png]({{ site.baseurl }}/images/Blog-3-Sec-3-Vis-1.png)

And now we will do the same, but for rotated dogs.

```python
rrot = tf.keras.Sequential([layers.experimental.preprocessing.RandomRotation(0.2)])

for image, label in train_dataset.take(1):
  first_image = image[0]

image = tf.expand_dims(first_image, 0)

plt.figure(figsize=(10, 10))
for i in range(9):
  flipimage = rrot(image, training = True)
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(flipimage[0] / 255)
  plt.axis("off")
```

![Blog-3-Sec-3-Vis-2.png]({{ site.baseurl }}/images/Blog-3-Sec-3-Vis-2.png)

And now, we will incorporate these layers into our model2.  Below is the code for the model, to compile it, and then to fit the model.

```python
model2 = tf.keras.Sequential([
 layers.RandomFlip(),
 layers.RandomRotation(0.2),
 layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(32, (3, 3), activation='relu'),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(64, (3, 3), activation='relu'), # n "pixels" x n "pixels" x 64
 layers.Flatten(), # n^2 * 64 length vector
 layers.Dropout(0.2), # randomly "drop" or delete 20% of connections between the previous layer and the next layer
 layers.Dropout(0.2),
 layers.Dense(64, activation='relu'),
 layers.Dense(2)
])
```

```python
model2.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
```

```python
history = model2.fit(train_dataset, epochs = 20, validation_data = validation_dataset)
```

![Blog-3-m2-e1.png]({{ site.baseurl }}/images/Blog-3-m2-e1.png)
![Blog-3-m2-e2.png]({{ site.baseurl }}/images/Blog-3-m2-e2.png)

And this is a visualization of training and validation accuracy across epochs.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![Blog-3-m2-vis.png]({{ site.baseurl }}/images/Blog-3-m2-vis.png)

The validation accuracy of model2 was around **58%** during training.  This has slightly lower accuracy than model1, but overfitting is a lot better than in model1, while still prevalent, since the training accuracy was only 67%, or less than 10% higher than validation, whereas it was over 30% higher on model1.

## ยง4. Data Preprocessing

Next, we will use some data preprocessing layers.  The following code will save our model time by processing the data at the beginning so that it can spend more time training the dataset.

```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Now we will incorporate this into a new model3, then compile and fit model3.  Then we will show a visualization of the training versus validation accuracy across epochs.

```python
model3 = tf.keras.Sequential([
 preprocessor,
 layers.RandomFlip('horizontal'),
 layers.RandomRotation(0.2),
 layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(32, (3, 3), activation='relu'),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(64, (3, 3), activation='relu'), # n "pixels" x n "pixels" x 64
 layers.Flatten(), # n^2 * 64 length vector
 layers.Dropout(0.2), # randomly "drop" or delete 20% of connections between the previous layer and the next layer
 layers.Dropout(0.2),
 layers.Dense(64, activation='relu'),
 layers.Dense(2)
])
```

```python
model3.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
```

```python
history = model3.fit(train_dataset, epochs = 20, validation_data = validation_dataset)
```

![Blog-3-m3-e1.png]({{ site.baseurl }}/images/Blog-3-m3-e1.png)
![Blog-3-m3-e2.png]({{ site.baseurl }}/images/Blog-3-m3-e2.png)

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![Blog-3-m3-vis.png]({{ site.baseurl }}/images/Blog-3-m3-vis.png)

The validation accuracy of the model was **around 75%** during training.  This is much improved from our model1 accuracy, since that was under 65%, so this model is a large improvement.  There is now much less overfitting, as the training data has an accuracy of around 79%, only about 5% higher than the validation data, which is much better than the 30% higher from model1, and 10% higher from model2.

## ยง5. Transfer Learning

Now, on our last model, we will incorporate a pre-existing base model into our model.  This is the code to implement the base layer.

```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

Now, we will incorporate this layer into our model, but this time, we will only have a few other layers, since this is an extremely complex layer.  We will then compile the model and fit it to the training data.

```python
model4 = tf.keras.Sequential([
 preprocessor,
 layers.RandomFlip('horizontal'),
 layers.RandomRotation(0.2),
 base_model_layer,
 layers.Conv2D(32, (3, 3), activation='relu'),
 layers.Flatten(),
 layers.Dense(2)
])
```

```python
model4.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
```

```python
history = model4.fit(train_dataset, epochs = 20, validation_data = validation_dataset)
```

![Blog-3-m4-e1.png]({{ site.baseurl }}/images/Blog-3-m4-e1.png)
![Blog-3-m4-e2.png]({{ site.baseurl }}/images/Blog-3-m4-e2.png)

And now we create a visualization of the training and validation accuracy across epochs:

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![Blog-3-m4-vis.png]({{ site.baseurl }}/images/Blog-3-m4-vis.png)

The validation accuracy of this model . . .

## ยง6. Score on Test Data

Now, we will test our fitted model4, our best performing model, onto test data that it has not seen yet, as shown in the code below:

```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

The output here is 'Test accuracy : 0.xxxx', which indicates . . .