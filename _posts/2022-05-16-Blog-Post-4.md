---
layout: post
title: Fake News Classification
---

## §1. Acquire Training Data

We will start by importing all necessary modules:

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers, losses, Input, Model, utils
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization, StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import matplotlib.pyplot as plt

import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Next, we will obtain our data from this url:

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
# get data
df = pd.read_csv(train_url, index_col = 0)
# read into pandas df
```

## §2. Make a Dataset

We will start by importing nltk, the natural language toolkit, and other things so we can find English stopwords.

```python
from gensim.utils import simple_preprocess # lowercases, tokenizes, de-accents
import nltk
nltk.download()
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
```

Next, we will remove stop words from the data, removing them from both the titles and the text, and then put that new text into a tensorflow Dataset.

```python
def remove_stopwords(texts):
  # get rid of words in the stop_words list
  return [' '.join([word for word in str(doc).split() if word not in stop_words]) for doc in texts]

def make_dataset(data):
  # preprocess the dataset for feeding into the tensorflow model
  data['title'] = remove_stopwords(data['title']) #remove stopwords from titles
  data['text'] = remove_stopwords(data['text']) #remove stopwords from text
  data = tf.data.Dataset.from_tensor_slices( #process it into a tensorflow data
      (
        {
            "title" : data[["title"]], 
            "text" : data[["text"]]
        }, 
        {
            "fake" : data["fake"]
        }
    )
  )
  return data.batch(100)

dataset = make_dataset(df)
```

Now, we will split this data into training and validation datasets.

```python
train = dataset.take(int(0.8 * len(dataset)))
val   = dataset.skip(int(0.8 * len(dataset))).take(int(0.2 * len(dataset)))
```

To find the base rate, we will iterate through the train dataset and count how many fake news stories there are, and find the max percentage between fake and real news.

```python
trainpd = pd.DataFrame(train)
count = 0
totfake = 0
for i in trainpd[1]:
  for j in i.get('fake'):
    count += 1
    if int(j) == 0:
      totfake += 1
fakeper = totfake/count
print(max(fakeper, 1-fakeper))
```

This resulted in a base rate of 0.5221, or 52.21%.  We will use this as the basis for how well our models do later.

We will now create a text vectorization layer, to convert the text into numbers.

```python
#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

## §3. Create Models

Now, we will create our three models.  The processes for each model are very similar, so my explanations for Models 2 and 3 will be more brief.

### §3.1 Model 1

We start by creating a standardization funciton and vectorizing the titles with the title vectorization layer.

```python
#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data) # make data lower case
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation  # remove punctuation

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

Next, we create our model inputs that we will use as the inputs for the model.

```python
# inputs
titles_input = Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)
```

Now, we create the layers that we will use and specify the output of two parameters that will determine the probability that the title is of a fake news article.

```python
# layers for processing the titles
titles_features = title_vectorize_layer(titles_input)
titles_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embeddingtitle")(titles_features)
titles_features = layers.Dropout(0.2)(titles_features)
titles_features = layers.GlobalAveragePooling1D()(titles_features)
titles_features = layers.Dropout(0.2)(titles_features)
titles_features = layers.Dense(32, activation='relu')(titles_features)

output = layers.Dense(2, name = "fake")(titles_features) # output 2 prediction scores
```

Now, we create the model,

```python
model1 = Model(
    inputs = titles_input,
    outputs = output
)
```

we compile the model,

```python
model1.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```

and we fit the model on the training dataset.

```python
history1 = model1.fit(train, 
                    validation_data=val,
                    epochs = 25, 
                    verbose = True)
```

![Blog-4-m1-e1.png]({{ site.baseurl }}/images/Blog-4-m1-e1.png)
![Blog-4-m1-e2.png]({{ site.baseurl }}/images/Blog-4-m1-e2.png)

Model 1 had a validation rate of around 98%, which is very good.  In this model, I used an embedding layer, dropout layers and a pooling layer in order to categroize the words and to prevent overfitting, which this model seemed to do, since both the training and validation rates were around 985.

### §3.2 Model 2

We start by making a similar text vectorization layer, specifying the inputs, layers, and output.

```python
text_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

text_vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```

```python
# inputs
text_input = Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```

```python
# layers for processing the texts
text_features = title_vectorize_layer(text_input)
text_features = layers.Embedding(size_vocabulary, output_dim = 3, name="embeddingtext")(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)

output = layers.Dense(2, name = "fake")(text_features)
```

Then, we create, compile, and fit the model on the training data.

```python
model2 = Model(
    inputs = text_input,
    outputs = output
)
```

```python
model2.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```

```python
history2 = model2.fit(train, 
                    validation_data=val,
                    epochs = 25, 
                    verbose = True)
```

![Blog-4-m2-e1.png]({{ site.baseurl }}/images/Blog-4-m2-e1.png)
![Blog-4-m2-e2.png]({{ site.baseurl }}/images/Blog-4-m2-e2.png)

Similarly to Model 1, we used the same layers, but with the text data instead of the title data, and achieved similar results of around 98%, but overfitting was a little worse on thi model since the validation rate was slightly below the training rate in this model, whereas it was slightly above the training rate in Model 1.

### §3.3 Model 3

We start Model 3 by concatenating the titles_features and text_features, which combines the layers that we used in Models 1 and 2.  Then, we add another dense layer before our output to work with the combined dataset before making predictions.

```python
layer = layers.concatenate([titles_features, text_features], axis = 1)
layer = layers.Dense(32, activation='relu')(layer)
output = layers.Dense(2, name = 'fake')(layer)
```

And now, we will create, compile, and fit this model.

```python
# initialize model
model3 = Model(
    inputs = [titles_input, text_input],
    outputs = output
)
```

```python
model3.compile(optimizer="adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
```

```python
history3 = model3.fit(train, 
                    validation_data=val,
                    epochs = 25, 
                    verbose = True)
```

![Blog-4-m3-e1.png]({{ site.baseurl }}/images/Blog-4-m3-e1.png)
![Blog-4-m3-e2.png]({{ site.baseurl }}/images/Blog-4-m3-e2.png)

In this model, we had a validation rate of around 99%, which is similar to the training rate of 99%, so this model performed extremely well, even better than Models 1 and 2, and does not display significant overfitting.  Thus, this is the best model for predicting whether or not an article is fake news or not.

## §4. Model Evaluation

We start by reading in the data to a pandas DataFrame.

```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_pd = pd.read_csv(test_url, index_col = 0)
```

Then we use our make_dataset function to create the test dataset.

```python
test = make_dataset(test_pd)
```

And we evaluate Model 3.

```python
loss, accuracy = model3.evaluate(test)
print('Test accuracy :', accuracy)
```

![Blog-4-m3-eval.png]({{ site.baseurl }}/images/Blog-4-m3-eval.png)

Model 3 has a test accuracy of 99.19% on the test data, which it has not seen before.

## §5. Embedding Visualization

Now we will create an embedding visualization with model3's title embedding weights to shows which words were predictors of fake news.

```python
weights = model3.get_layer('embeddingtitle').get_weights()[0] # get the weights from the embedding layer
vocab = title_vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({ # create data frame of embedded weights
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```

```python
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = [2]*len(embedding_df),
                 hover_name = "word")

fig.show() # show plot
```

![Blog-4-embedding-viz.png]({{ site.baseurl }}/images/Blog-4-embedding-viz.png)

Some notable words in this visualization are 'video', 'wow', and 'liberal', which are on the far right side of the graph, meaning that they are probably more likely to be fake news, while 'says', 'factbox', 'speaker', and 'official' are towards the left side, indicating that they are more likely to be used in real stories.