---
layout: post
title: Fake News Classification
---

## §1. Acquire Training Data

We will start by importing all necessary modules:

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers, losses, Input, Model, utils
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization, StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import matplotlib.pyplot as plt

import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Next, we will obtain our data from this url:

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
# get data
df = pd.read_csv(train_url, index_col = 0)
# read into pandas df
```


## §2. First Model

We will create our first model using keras Sequential, and using many layers, including convolutional layers, max pooling layes, flattening, dropout, and dense layers.  These layers each perform different operations, as specified in the comments of the block below.

```python
model1 = tf.keras.Sequential([
 layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(32, (3, 3), activation='relu'),
 layers.MaxPooling2D((2, 2)), # take maximum of 2x2 blocks
 layers.Conv2D(64, (3, 3), activation='relu'), # n "pixels" x n "pixels" x 64
 layers.Flatten(), # n^2 * 64 length vector
 layers.Dropout(0.2), # randomly "drop" or delete 20% of connections between the previous layer and the next layer
 layers.Dropout(0.2),
 layers.Dense(64, activation='relu'),
 layers.Dense(2) # output 2 predictors
])
```

Next, we will compile our model.

```python
model1.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
```

Then, we use this code to fit the model.

```python
history = model1.fit(train_dataset, epochs = 20, validation_data = validation_dataset)
```

![Blog-3-m1-e1.png]({{ site.baseurl }}/images/Blog-3-m1-e1.png)
![Blog-3-m1-e2.png]({{ site.baseurl }}/images/Blog-3-m1-e2.png)

And this is a visualization of training and validation accuracy across epochs.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![Blog-3-m1-vis.png]({{ site.baseurl }}/images/Blog-3-m1-vis.png)

The accuracy of the model stabilized **between 60% and 65%** during training.  This is about 5-10% better than the baseline, so this model is moderately better than the baseline.  We observe very high overfitting in model1, as seen through our training accuracy, which ended up around 98%, so there is a gap of over 30%, which indicates very severe overfitting.

## §3. Model with Data Augmentation

Now we will use data augmentation, where we make more data points by rotating and flipping the images we have, since a rotated or flipped cat is still a cat.  Below is an example of some flipped cats.

```python
rflip = tf.keras.Sequential([layers.experimental.preprocessing.RandomFlip()])
# create layer that will flip image
for image, label in train_dataset.take(1):
  first_image = image[0] 
# get image from dataset
image = tf.expand_dims(first_image, 0)
# expand dimensions of image
plt.figure(figsize=(10, 10))
for i in range(9):
  flipimage = rflip(image, training = True)
  # flip image
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(flipimage[0] / 255)
  plt.axis("off")
  # plot image
```

![Blog-3-Sec-3-Vis-1.png]({{ site.baseurl }}/images/Blog-3-Sec-3-Vis-1.png)

And now we will do the same, but for rotated dogs.

```python
rrot = tf.keras.Sequential([layers.experimental.preprocessing.RandomRotation(0.2)])
# create layer that will rotate image
for image, label in train_dataset.take(1):
  first_image = image[0]
# get image from dataset
image = tf.expand_dims(first_image, 0)
# expand dimensions of image
plt.figure(figsize=(10, 10))
for i in range(9):
  flipimage = rrot(image, training = True)
  # rotate image
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(flipimage[0] / 255)
  plt.axis("off")
  # plot image
```

![Blog-3-Sec-3-Vis-2.png]({{ site.baseurl }}/images/Blog-3-Sec-3-Vis-2.png)

And now, we will incorporate these layers into our model2.  Below is the code for the model, to compile it, and then to fit the model.

```python
model2 = tf.keras.Sequential([
 layers.RandomFlip(), # random flips
 layers.RandomRotation(0.2), # random rotations
 layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(32, (3, 3), activation='relu'),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(64, (3, 3), activation='relu'),
 layers.Flatten(),
 layers.Dropout(0.2),
 layers.Dropout(0.2),
 layers.Dense(64, activation='relu'),
 layers.Dense(2)
])
```

```python
model2.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
```

```python
history = model2.fit(train_dataset, epochs = 20, validation_data = validation_dataset)
```

![Blog-3-m2-e1.png]({{ site.baseurl }}/images/Blog-3-m2-e1.png)
![Blog-3-m2-e2.png]({{ site.baseurl }}/images/Blog-3-m2-e2.png)

And this is a visualization of training and validation accuracy across epochs.

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![Blog-3-m2-vis.png]({{ site.baseurl }}/images/Blog-3-m2-vis.png)

The validation accuracy of model2 was around **58%** during training.  This has slightly lower accuracy than model1, but overfitting is a lot better than in model1, while still prevalent, since the training accuracy was only 67%, or less than 10% higher than validation, whereas it was over 30% higher on model1.

## §4. Data Preprocessing

Next, we will use some data preprocessing layers.  The following code will save our model time by processing the data at the beginning so that it can spend more time training the dataset.

```python
i = tf.keras.Input(shape=(160, 160, 3))
x = tf.keras.applications.mobilenet_v2.preprocess_input(i)
preprocessor = tf.keras.Model(inputs = [i], outputs = [x])
```

Now we will incorporate this into a new model3, then compile and fit model3.  Then we will show a visualization of the training versus validation accuracy across epochs.

```python
model3 = tf.keras.Sequential([
 preprocessor, # processes data before training
 layers.RandomFlip('horizontal'),
 layers.RandomRotation(0.2),
 layers.Conv2D(32, (3, 3), activation='relu', input_shape=(160, 160, 3)),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(32, (3, 3), activation='relu'),
 layers.MaxPooling2D((2, 2)),
 layers.Conv2D(64, (3, 3), activation='relu'),
 layers.Flatten(),
 layers.Dropout(0.2),
 layers.Dropout(0.2),
 layers.Dense(64, activation='relu'),
 layers.Dense(2)
])
```

```python
model3.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
```

```python
history = model3.fit(train_dataset, epochs = 20, validation_data = validation_dataset)
```

![Blog-3-m3-e1.png]({{ site.baseurl }}/images/Blog-3-m3-e1.png)
![Blog-3-m3-e2.png]({{ site.baseurl }}/images/Blog-3-m3-e2.png)

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![Blog-3-m3-vis.png]({{ site.baseurl }}/images/Blog-3-m3-vis.png)

The validation accuracy of the model was **around 75%** during training.  This is much improved from our model1 accuracy, since that was under 65%, so this model is a large improvement.  There is now much less overfitting, as the training data has an accuracy of around 79%, only about 5% higher than the validation data, which is much better than the 30% higher from model1, and 10% higher from model2.

## §5. Transfer Learning

Now, on our last model, we will incorporate a pre-existing base model into our model.  This is the code to implement the base layer.

```python
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
base_model.trainable = False

i = tf.keras.Input(shape=IMG_SHAPE)
x = base_model(i, training = False)
base_model_layer = tf.keras.Model(inputs = [i], outputs = [x])
```

Now, we will incorporate this layer into our model, but this time, we will only have a few other layers, since this is an extremely complex layer.  We will then compile the model and fit it to the training data.

```python
model4 = tf.keras.Sequential([
 preprocessor,
 layers.RandomFlip('horizontal'),
 layers.RandomRotation(0.2),
 base_model_layer, # incorporates pre existing model layer
 layers.Conv2D(32, (3, 3), activation='relu'),
 layers.Flatten(),
 layers.Dense(2)
])
```

```python
model4.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer='adam',
              metrics=['accuracy'])
```

```python
history = model4.fit(train_dataset, epochs = 20, validation_data = validation_dataset)
```

![Blog-3-m4-e1.png]({{ site.baseurl }}/images/Blog-3-m4-e1.png)
![Blog-3-m4-e2.png]({{ site.baseurl }}/images/Blog-3-m4-e2.png)

And now we create a visualization of the training and validation accuracy across epochs:

```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

![Blog-3-m4-vis.png]({{ site.baseurl }}/images/Blog-3-m4-vis.png)

The validation accuracy of this model was around **97% accuracy** on the training data.  This is the best validation accuracy by far, getting very close to 100%, as opposed to all previous models having accuracies below 75%.  In addition, overfitting does not seem to be a problem here since the training data accuracy is only about 1% higher at 98%, as opposed to the other models, where there was a much higher difference between the training and validation accuracies.

## §6. Score on Test Data

Now, we will test our fitted model4, our best performing model, onto test data that it has not seen yet, as shown in the code below:

```python
loss, accuracy = model4.evaluate(test_dataset)
print('Test accuracy :', accuracy)
```

The output here is 'Test accuracy : 0.9792', which indicates that the model did very well, achieving almost 98% accuracy on the testing data.