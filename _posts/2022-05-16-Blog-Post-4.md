---
layout: post
title: Fake News Classification
---

## ยง1. Acquire Training Data

We will start by importing all necessary modules:

```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers, losses, Input, Model, utils
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization, StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import matplotlib.pyplot as plt

import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

Next, we will obtain our data from this url:

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
# get data
df = pd.read_csv(train_url, index_col = 0)
# read into pandas df
```

## ยง2. Make a Dataset

We will start by importing nltk, the natural language toolkit, and other things so we can find English stopwords.

```python
from gensim.utils import simple_preprocess # lowercases, tokenizes, de-accents
import nltk
nltk.download()
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
```

Next, we will remove stop words from the data, removing them from both the titles and the text, and then put that new text into a tensorflow Dataset.

```python
def remove_stopwords(texts):
  # get rid of words in the stop_words list
  return [' '.join([word for word in str(doc).split() if word not in stop_words]) for doc in texts]

def make_dataset(data):
  # preprocess the dataset for feeding into the tensorflow model
  data['title'] = remove_stopwords(data['title']) #remove stopwords from titles
  data['text'] = remove_stopwords(data['text']) #remove stopwords from text
  data = tf.data.Dataset.from_tensor_slices( #process it into a tensorflow data
      (
        {
            "title" : data[["title"]], 
            "text" : data[["text"]]
        }, 
        {
            "fake" : data["fake"]
        }
    )
  )
  return data.batch(100)

dataset = make_dataset(df)
```

Now, we will split this data into training and validation datasets.

```python
train = dataset.take(int(0.8 * len(dataset)))
val   = dataset.skip(int(0.8 * len(dataset))).take(int(0.2 * len(dataset)))
```

To find the base rate, we will iterate through the train dataset and count how many fake news stories there are, and find the max percentage between fake and real news.

```python
trainpd = pd.DataFrame(train)
count = 0
totfake = 0
for i in trainpd[1]:
  for j in i.get('fake'):
    count += 1
    if int(j) == 0:
      totfake += 1
fakeper = totfake/count
print(max(fakeper, 1-fakeper))
```

This resulted in a base rate of 0.5221, or 52.21%.  We will use this as the basis for how well our models do later.

We will now create a text vectorization layer, to convert the text into numbers.

```python
#preparing a text vectorization layer for tf model
size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

title_vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

title_vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
```

## ยง3. Create Models

### ยง3.1 Model 1