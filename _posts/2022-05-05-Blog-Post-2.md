---
layout: post
title: IMDB Scraper
---

In this blog post, we are going to create a scraper that scrapes information from the IMDB website.  Here is a link to the repository with this code:  https://github.com/NicholasDelianedis/HW2Scrape

## ยง1. Write a Scraper

We will start by importing the required modules, specifically scrapy.

```python
import scrapy
from scrapy.http import Request
```

Next, we will create our class to put our parse functions to scrape the website.  We will start by adding a name, in this case 'imdb_spider', which we will call our spider, and we have a list called start_urls which in this case contains one IMDB url that we will use to start our scrape.

```python
class ImdbSpider(scrapy.Spider):
    name = 'imdb_spider'
    
    start_urls = ['https://www.imdb.com/title/tt0898266/']

```

Now, we will create three parse functions in this class.  The first function will be called 'parse', and will direct the scraper to the full credits page of the IMDB page, and use that to call the next parse function.  It does this by creating a variable 'new_url', which is just the start_url with 'fullcredits' added on the end.  Then the function yields a Request object that calls the next function, 'parse_full_credits' with the new_url we obtained earlier.

```python
    def parse(self, response):
        """
        Directs scraper from movie/show page to its credits page
        """
        new_url = response.url + "fullcredits" # obtain credits page url
        yield Request(new_url, callback = self.parse_full_credits)
        # call parse_full_credits with new url
```

The next parse function, 'parse_full_credits', will start with the url of the credits page, and then use CSS selectors to obtain the end of the url address of each actor's IMDB page through the image shown on the full credits page.  Then, for each actor, this function adds the suffix onto the current url to obtain the actor's url, and calls the last parse function, 'parse_actor_page', with the actor's IMDB url.

```python
    def parse_full_credits(self, response):
        """
        Direct scraper to each actor on the credits page by calling the
        url from each actor's picture
        """
        for i in [a.attrib["href"] for a in response.css("td.primary_photo a")]:
            new_url = response.urljoin(i) # obtain actor's url page
            yield Request(new_url, callback = self.parse_actor_page)
            # call parse_actor_page on each actor's url page
```

The last parse function, 'parse_actor_page', starts on an actor's IMDB page, then obtains the actor's name through CSS selectors.  Then, for each movie or TV show that they have credits for, the function gets the name of the movie or TV show through CSS selectors, and yields a dictionary with the actor's name and the movie or TV show name.  Then, when this function is called, this will create a csv file with all of the dictionaries yielded here.

```python
    def parse_actor_page(self, response):
        """
        Scrape actor page by extracting actor name and movie or tv show
        name for each movie or tv show actor was involved in
        """
        actor = response.css("div.name-overview-widget h1.header span.itemprop::text").get()
        # use css selectors to get the actor's name
        for element in response.css("div.filmo-row"):
            movietv = element.css("div.filmo-row b a::text").get()
            # obtain movie/tv show name through css selectors
            yield {
                "actor" : actor, 
                "movie_or_TV_name" : movietv
            }
            # yield a dict with the actor name and movie/tv show name
```

Thus, the end result will be a csv file with a series of dictionaries, one for each movie or TV show an actor is in, for each actor in the show the original url referred to.

## ยง2. Analyze the Results

Now, we will try to analyze the data we got from the scraper.  First, we will put the data into a Pandas DataFrame, since they are easy to work with.  We will import pandas, read the csv, then rename the 'movie_or_TV_name' to 'movie' for simplicity.

```python
import pandas as pd # import pandas

results = pd.read_csv('results.csv')
results["movie"] = results["movie_or_TV_name"]
```

Next, we will create a table with each movie and a count of the number of actors shared with the show we scraped by grouping the data by movie, and getting a list of movies and their shared actor count, then converting that into another Pandas DataFrame.  Then, we sort by number of shared actors, and display the top 10, as shown below the code

```python
resultcount = pd.DataFrame({'movie':results.groupby("movie").size().index, 'number of shared actors':results.groupby("movie").size().values})
# create Pandas DataFrame of movies and number of shared actors
top10 = resultcount.sort_values(["number of shared actors"], ascending = False)
# sort the results by number of shared actors
top10[:10] # display the top 10
```

![Blog-2-Image-1.png]({{ site.baseurl }}/images/Blog-2-Image-1.png)